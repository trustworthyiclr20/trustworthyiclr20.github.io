<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICLR 2020 Workshop on Towards Trustworthy ML: Rethinking Security and Privacy for ML">

  <title>ICLR 2020 Workshop on Towards Trustworthy ML: Rethinking Security and Privacy for ML</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5">Towards Trustworthy ML:<br /> <i>Rethinking Security and Privacy for ML</i></h1>
  <h2>ICLR 2020 Workshop</h2>
  <p class="mb-0"><b>Date:</b> April 26, 2020 (Sunday)</p>
  <p class="mb-0"><b>Location:</b> Online <s>Millennium Hall, Addis Ababa, Ethiopia (co-located with <a href="https://iclr.cc/" target="_blank">ICLR 2020</a>)</s></p>
  <p class="mb-0"><b>Contact:</b> trustworthyiclr20@googlegroups.com (this will email all organizers)</p>

<br/>

<h2>How to participate?</h2>

The virtual workshop will consist of a mix of pre-recorded and live content, with multiple different ways to participate!

<ul>
  <li><a href="https://iclr.cc/Register/view-registration" target="_default">Register for ICLR</a> and <a href="https://iclr.cc/virtual/workshops_6.html" target="_blank">access the virtual workshop on ICLR's virtual conference website</a>. This page includes the livestream.</li>
  <li><a href="https://app.sli.do/event/hujo7qa1" target="_default">Suggest and vote on panel discussion questions</a> ahead and during the panel.</li>
  <li>Attend the live poster sessions with breakout rooms for each poster (Zoom links on schedule below under the list of <a href="#accepted">"Accepted papers"</a>)</li>
  <li><a href="https://iclr.rocket.chat/channel/workshop_trustworthy" target="_default">Join the Rocket Chat</a> to meet and discuss with other workshop attendees throughout the day.</li>
</ul>

  
<h5>Streaming</h5>

<p>Throughout the day we will be streaming (recorded) invited talks, (recorded) contributed talks, and a (live) panel discussion. Please see the Schedule for details on what is being (live)streamed and when. During the (live)stream, please also join the Rocket Chat to discuss the workshop content with other participants.</p>


<h5>Live panel</h5>

<p>Our invited speakers and panelists will discuss, live, a mix of curated and audience questions. Please <a href="https://app.sli.do/event/hujo7qa1" target="_default">suggest and vote on panel discussion questions</a> to help us put together the best discussion possible!</p>

<h5>Poster Session</h5>

<p>For each paper being presented at the workshop, we will host (1) the pre-recorded presentation from SlidesLive and (2) a Zoom breakout room during the poster session. The Zoom breakout rooms will be open only during the poster session timeslot (see the Schedule), during which authors will join the meeting rooms to allow you to ask them questions face-to-face. 
</p>



  <h2>Schedule (Eastern time)</h2>

<p>Rows highlighted in green are <span class="badge badge-success">LIVE</span> whereas rows highlighted in yellow are <span class="badge badge-warning">PRE-RECORDED</span>. The poster session is distributed across multiple <span class="badge badge-primary">ZOOM</span> meetings, found in the list of <a href="#accepted">"Accepted papers"</a> below.</p>

  <table class="table table-sm">
    <tbody>
    <tr class="table-success">
      <th scope="row">8:45am</th>
      <td>Live opening remarks (Nicolas Papernot)</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Livestreaming</a></td>
    </tr>    <tr class="table-warning">
      <th scope="row">8:50am</th>
      <td>Zico Kolter: TML_3 Beyond "provable" robustness: new directions in adversarial robustness</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">9:30am</th>
      <td>Lujo Bauer: TML_0 On the Susceptibility to Adversarial Examples Under Real-World Constraints</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">10:10am</th>
      <td>Seeta Pena Gangadharan: TML_1 Context, Research, Refusal: Perspectives on Abstract Problem-Solving</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">10:50am</th>
      <td>Timnit Gebru: TML_2 Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-success">
      <th scope="row">11:30am</th>
      <td>Live Panel moderated by Ram Shankar: <ul>
            <li>Zico Kolter</li>
            <li>Lujo Bauer</li>
            <li>Seeta Pena Gangadharan</li>
            <li>Timnit Gebru</li>
            <li>Justin Gilmer</li>
          </ul></td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Livestreaming</a><br/><br/><a href="https://app.sli.do/event/hujo7qa1" target="_default">Ask + vote questions</a>
      </td>
    </tr>
    <tr><th scope="row" colspan="3">Break</th></tr>
    <tr class="table-success">
      <th scope="row">1:00pm</th>
      <td>Live poster session with all authors of accepted papers (list of Zoom links below under <a href="#accepted">"Accepted papers"</a>.)</td>
      <td></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">3:00pm</th>
      <td>TML_4 Increasing the robustness of DNNs against image corruptions by playing the Game of Noise</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">3:20pm</th>
      <td>TML_5 Bounding Singular Values of Convolution Layers</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">3:40pm</th>
      <td>TML_6 Revisiting Ensembles in an Adversarial Context: Improving Natural Accuracy</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">4:00pm</th>
      <td>TML_7 Games for Fairness and Interpretability</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">4:20pm</th>
      <td>TML_8 Output Diversified Initialization for Adversarial Attacks</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">4:40pm</th>
      <td>TML_9 On the Benefits of Models with Perceptually-Aligned Gradients</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">5:00pm</th>
      <td>TML_10  Randomized Smoothing of All Shapes and Sizes</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">5:20pm</th>
      <td>TML_11  On Pruning Adversarially Robust Neural Networks</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">5:40pm</th>
      <td>TML_12  DADI: Dynamic Discovery of Fair Information with Adversarial Reinforcement Learning</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">6:00pm</th>
      <td>TML_13  Attacking Neural Text Detectors</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">6:20pm</th>
      <td>TML_14  Black-Box Smoothing: A Provable Defense for Pretrained Classifiers</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">6:40pm</th>
      <td>TML_15  Privacy-preserving collaborative machine learning on genomic data using TensorFlow</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">7:00pm</th>
      <td>TML_16  Preventing backdoor attacks via Student-Teacher Ensemble Training</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">7:20pm</th>
      <td>TML_17  Politics of Adversarial Machine Learning</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
    <tr class="table-warning">
      <th scope="row">7:40pm</th>
      <td>TML_18  Improved Wasserstein Attacks and Defenses</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>
        <tr class="table-warning">
      <th scope="row">8:00pm</th>
      <td>TML_19  ADVERSARIAL ROBUSTNESS IN DATA AUGMENTATION</td>
      <td><a href="https://iclr.cc/virtual/workshops_6.html">Streaming</a></td>
      <td><a href="https://vectorinstitute.zoom.us/j/97327541071?pwd=dEh2WXFMZm95V1k5TjVYT3c2cG5hUT09">Hallway Track Zoom</a></td>
    </tr>



    <tr>
      <th scope="row">End</th>
      <td></td>
      <td>-</td>
    </tr>
    </tbody>
  </table>



<h2 id="accepted">Accepted papers</h2>

<div class="alert alert-danger" role="alert">
  We are in the process of setting up Zoom links for the poster session!
</div>

<ul>
<li><strong> Increasing the robustness of DNNs against imagecorruptions by playing the Game of Noise</strong> Evgenia Rusak (University of Tuebingen); Lukas Schott (Max Planck Institute for Intelligent Systems and University of Tuebingen); Roland S. Zimmermann (University of Tuebingen); Julian Bitterwolf (University of Tuebingen); Oliver Bringmann (University of Tuebingen); Matthias Bethge (University of Tübingen); Wieland Brendel (University of Tuebingen)[<a href="rusak.pdf">paper</a>]</li>
<li><strong>Bounding Singular Values of Convolution Layers</strong> Sahil Singla (University of Maryland); Soheil Feizi (University of Maryland)<!--[<a href="LINK">paper</a>]--></li>
<li><strong>Revisiting Ensembles in an Adversarial Context: Improving Natural Accuracy</strong> Aditya Saligrama (MIT PRIMES); Guillaume Leclerc (MIT)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://mit.zoom.us/j/91041424279" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>Games for Fairness and Interpretability</strong> Eric Chu (Massachusetts Institute of Technology); Nabeel N Gillani (Massachusetts Institute of Technology); Sneha Priscilla Makini (Massachusetts Institute of Technology)[<a href="chu.pdf">paper</a>]</li>
<li><a href="https://stanford.zoom.us/j/95876387116" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>Output Diversified Initialization for Adversarial Attacks</strong> Yusuke Tashiro (Stanford University); Yang Song (Stanford University); Stefano Ermon (Stanford)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://stanford.zoom.us/j/7903543554" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>On the Benefits of Models with Perceptually-Aligned Gradients</strong> Gunjan Aggarwal (Adobe); Abhishek Sinha (Stanford); Mayank Singh (Adobe Systems); Nupur Kumari (Adobe Systems)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_NTFjMGVhMDgtZGY2ZS00ZTA5LWJkMDYtMmYxZjRiYjI3NTNl%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%2297250d9a-bac4-4097-8f57-0173feeba1cf%22%7d" class="badge badge-primary">Teams for poster session</a>&nbsp;<strong>Randomized Smoothing of All Shapes and Sizes</strong> Greg Yang (Microsoft Research AI); Tony Duan (Microsoft Research); J. Edward Hu (Microsoft Research AI); Hadi Salman (Microsoft Research); Ilya Razenshteyn (Microsoft Research); Jerry Li (Microsoft)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://princeton.zoom.us/j/3871410688" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>On Pruning Adversarially Robust Neural Networks</strong> Vikash Sehwag (Princeton University); Shiqi Wang (Columbia University); Prateek Mittal (Princeton University); Suman Jana (Columbia University)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://mit.zoom.us/j/95984464168" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>DADI: Dynamic Discovery of Fair Information with Adversarial Reinforcement Learning</strong> Michiel A Bakker (MIT); Duy Patrick Tu (MIT); Humberto Riveron Valdes (MIT); Krishna Gummadi (MPI-SWS); Kush R Varshney (IBM Research); Adrian Weller (University of Cambridge); Alex `Sandy' Pentland (MIT)<!--[<a href="LINK">paper</a>]--></li>
<li><strong>Attacking Neural Text Detectors</strong> Maximilian Wolff (Viewpoint School)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTg1MjIzODMtYzU3MS00NDQ0LTk4NTMtNjUzZmMyMDc5ZjRm%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%22b6d20c24-850f-4e04-a41b-76c018b2781a%22%7d" class="badge badge-primary">Teams for poster session</a>&nbsp;<strong>Black-Box Smoothing: A Provable Defense for Pretrained Classifiers</strong> Hadi Salman (Microsoft Research AI); Mingjie Sun (Carnegie Mellon University); Greg Yang (Microsoft Research AI); Ashish Kapoor (Microsoft); Zico Kolter (Carnegie Mellon University)<!--[<a href="LINK">paper</a>]--></li>
<li><strong>Privacy-preserving collaborative machine learning on genomic data using TensorFlow</strong> Cheng Hong (Alibaba Group); Zhicong Huang (Alibaba Group); Wen-jie Lu (Alibaba Group); Hunter Qu (Alibaba Group); Li Ma (Alibaba Health); Morten Dahl (Dropout Labs); Jason Mancuso (Dropout Labs)[<a href="https://arxiv.org/pdf/2002.04344.pdf">paper</a>]</li>
<li><a href="https://mit.zoom.us/j/98252708851" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>Preventing backdoor attacks via Student-Teacher Ensemble Training</strong> Nur Muhammad (Mahi) Shafiullah (Massachusetts Institute of Technology); Shibani Santurkar (MIT); Dimitris Tsipras (MIT); Aleksander Madry (MIT)<!--[<a href="LINK">paper</a>]--></li>
<li><a href="https://harvard.zoom.us/j/98330811190?pwd=L2VyTU0ra0hmSlljTzY0NGZQRVJPdz09" class="badge badge-primary">Zoom for poster session</a>&nbsp;<strong>Politics of Adversarial Machine Learning</strong> Kendra Albert (Harvard Law School ); Jonathon Penney (Citizen Lab (University of Toronto) / Dalhousie / Princeton CITP); Bruce Schneier (Belfer Center for Science and International Affairs, Harvard Kennedy School.); Ram Shankar Siva Kumar (Microsoft (Azure Security))[<a href="albert.pdf">paper</a>]</li>
<li><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_N2E3ZDA5MmEtMTM5Mi00YzJjLWIyY2QtOWE3NWU5YzRiYjk2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%229c6d0839-fecf-43e4-81c9-d21788524abe%22%7d" class="badge badge-primary">Teams for poster session</a>&nbsp; <strong>Improved Wasserstein Attacks and Defenses</strong> J. Edward Hu (Microsoft Research AI); Greg Yang (Microsoft Research AI); Adith Swaminathan (Microsoft Research); Hadi Salman (Microsoft Research)<!--[<a href="LINK">paper</a>]--></li>
<li><strong>ADVERSARIAL ROBUSTNESS IN DATA AUGMENTATION</strong> Hamid Eghbal-zadeh (LIT AI Lab & Johannes Kepler University, Institute of Computational Perception); Khaled Koutini (Johannes Kepler University); Verena Haunschmid (Johannes Kepler University Linz); Paul Primus (Johannes Kepler University); Michal Lewandowski (Software Competence Center Hagenberg); Werner Zellinger (Software Competence Center Hagenberg); Gerhard Widmer (Johannes Kepler University)[<a href="eghbal.pdf">paper</a>]</li>
</ul>



  <h2>Sponsor</h2>
  <p>Thank you to the Open Philanthropy Project for sponsoring this event. Their grant will fund a best paper award.</p>

 <h2>Abstract</h2>
  <p>
    <i>Abstract</i>—As ML systems are pervasively deployed, security and privacy challenges became central to their design. The community produced a vast amount of work to address these challenges and increase trust in ML. Yet, much of it concentrates on well-defined problems that enable nice tractability from a mathematical perspective but are hard to translate to the threats that target real-world systems.
  </p>
  <p>
    This workshop calls for novel research that addresses the security and privacy risks arising from the deployment of ML, from malicious exploitation of vulnerabilities (e.g., adversarial examples or data poisoning) to concerns on fair, ethical and privacy-preserving uses of data. We aim to provide a home to new ideas “outside the box”, even if proposed preliminary solutions do not match the performance guarantees of known techniques. We believe that such ideas could prove invaluable to more effectively spur new lines of research that make ML more trustworthy.
  </p>
    <p>
    We aim to bring together experts from a variety of communities (ML, computer security, data privacy, fairness & ethics) in an effort to synthesize promising ideas and research directions, as well as foster and strengthen cross-community collaborations. Indeed, many fundamental problems studied in these diverse areas can be broadly recast as questions around the (in-)stability of ML models: generalization in ML, model memorization in privacy, adversarial examples in security, model bias in fairness and ethics, etc. Problems that we hope to encourage progress on are:
  </p>
    <p>
    <b>(#1) Adversarial robustness beyond Lp balls.</b> Recent years have seen a tremendous amount of research devoted to making ML models robust to small test-time perturbations sampled adversarially from an Lp-ball. While seemingly simple, this has proven a difficult challenge that remains mostly unsolved today. Yet, even if robustness in an Lp ball were to be achieved, complete model robustness would still be far from guaranteed. We encourage researchers to move beyond this “toy” problem to characterize the robustness of real-world systems for which adversarial examples pose a threat (e.g., malware detection, visual ad-blocking, voice assistants, etc...). We hope that specificities of these systems and of their deployments may point towards alternative—and more easily attainable—avenues towards secure inference. 
  </p>
    <p>
    <b>(#2) Stateful robustness.</b> Current adversarial example research focuses on securing a classifier for all possible use cases. This has proven to be extremely difficult and to date few solutions come close. However, when deployed, ML classifiers are not stateless systems that must respond to arbitrary inputs. Can we make use of additional knowledge (e.g., by making the classifier stateful, or by tailoring the defense to be deployed in one setting) which improves our ability to design defenses? Further, it might also be useful to think about ways to ensure graceful degradation of classifier performance in critical applications. For instance, instead of aiming to obtain robust classifiers that always accurately predict, it might be sufficient to get models that can fail gracefully (e.g., say “don’t know” or “the class is either cat or dog”).
  </p>
    <p>
    <b>(#3) ML techniques tailored for privacy.</b> Current approaches in the literature “tailor” privacy solutions to ML. Whether based on cryptography (e.g., homomorphic encryption) or statistical tools (e.g., differential privacy), they often aim to add privacy to existing ML techniques. We believe that the orthogonal approach, of designing new ML models or algorithms that are better suited for privacy-preserving techniques, is heavily underrepresented. We hope to encourage preliminary explorations in this space, even if they currently fail to reach state-of-the-art results.
  </p>
    <p>
    <b>(#4) Incentives in ML fairness and ethics.</b> Current approaches to ML fairness and ethics assume that the ML model owner is willing to collaborate and implement proposed solutions. However, the owner does not always have the incentives, the knowledge, or the means, to implement these solutions. We encourage the community to think about solutions that consider the model owner as adversarial and attempt to increase fairness “from the outside” of the model, e.g., modifying its inputs during training or inference. As part of this reflection, we hope submissions to the workshop will challenge existing definitions of ethics in machine learning.
  </p>
    <p>
    <b>(#5) Friendly uses of adversarial ML.</b> Adversarial ML is usually considered negative. This stems from the assumption that model owners are honest and ethical. However, ML is deployed in many real-world scenarios with questionable motives (e.g., privacy-invasive applications, social sorting). In such scenarios, adversarial machine learning may become a golden standard to protect users and communities. We welcome applications of adversarial techniques used to build solutions that help combating unethical machine learning applications.
  </p>



  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <div class="col-lg-1"></div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="nicolas.jpg" height="100px">
      <p>Nicolas Papernot<br /> [Chair]<br /> Google Brain</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="florian.png" height="100px">
      <p>Florian Tramer<br /> [Co-chair]<br /> Stanford University</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="carmela.jpg" height="100px">
      <p>Carmela Troncoso<br /> EPFL</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="nicholas.jpg" height="100px">
      <p>Nicholas Carlini<br /> Google Brain</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="shibani.jpg" height="100px">
      <p>Shibani Santurkar<br /> MIT</p>
    </div>
    <div class="col-lg-1"></div>
  </div>
	
	




<h2>Program Committee</h2>

<ul>
  <li>Adria Gascon  (The Alan Turing Institute)</li>
  <li>Akshayvarun Subramanya  (University of Maryland, Baltimore County)</li>
  <li>Anand Sarwate (Rutgers University)</li>
  <li>Aniruddha Saha (University of Maryland, Baltimore County)</li>
  <li>Anish Athalye (Massachusetts Institute of Technology)</li>
  <li>Asia  Biega (Max Planck Institute for Informatics)</li>
  <li>Aurélien  Bellet  (INRIA)</li>
  <li>Aylin Caliskan  (George Washington University)</li>
  <li>Berkay  Celik (Purdue University)</li>
  <li>Bogdan  Kulynych  (EPFL)</li>
  <li>Catuscia  Palamidessi (Laboratoire d'informatique de l'École polytechnique)</li>
  <li>Congzheng Song  (Cornell University)</li>
  <li>Dan Hendrycks (UC Berkeley)</li>
  <li>Dimitris  Tsipras (Massachusetts Institute of Technology)</li>
  <li>Earlence  Fernandes (University of Wisconsin-Madison)</li>
  <li>Eric  Wong  (Carnegie Mellon University)</li>
  <li>Fartash Faghri  (University of Toronto)</li>
  <li>Giovanni  Cherubin  (EPFL)</li>
  <li>Hadi  Salman  (Microsoft research)</li>
  <li>Jamie Hayes (University College London)</li>
  <li>Jason Martin (Intel Corporation)</li>
  <li>Jerry Li  (Microsoft Research)</li>
  <li>Jonas Rauber  (Max Planck Research School for Intelligent Systems)</li>
  <li>Julius  Adebayo (Massachusetts Institute of Technology)</li>
  <li>Kassem  Fawaz (University of Wisconsin-Madison)</li>
  <li>Kathrin Grosse  (CISPA Helmholtz Center / SIC)</li>
  <li>Kristian  Lum (Human Rights Data Analysis Group)</li>
  <li>Mahmood Sharif (Carnegie Mellon University)</li>
  <li>Maksym  Andriushchenko (EPFL)</li>
  <li>Matthew Jagielski (Northeastern University)</li>
  <li>Octavian  Suciu (University of Maryland)</li>
  <li>Pin-Yu  Chen  (IBM Research  AI)</li>
  <li>Sanghyun  Hong  (University of Maryland, College Park)</li>
  <li>Seda  Guerses (KU Leuven)</li>
  <li>Shruti  Tople (Microsoft Research)</li>
  <li>Shuang  Song  (Google)</li>
  <li>Sven  Gowal (DeepMind)</li>
  <li>Varun Chandrasekaran  (University of Wisconsin-Madison)</li>
  <li>Yair  Zick  (National University of Singapore)</li>
  <li>Yang  Zhang (CISPA Helmholtz Center for Information Security)</li>
  <li>Yizheng Chen  (Columbia University)</li>
</ul>




<h2>Call For Papers</h2>

 <p class="mb-0"><b>Submission deadline:</b> February 12th, 2020 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors: </b>February 25, 2020</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://cmt3.research.microsoft.com/ICLRTML2020/" target="_blank">https://cmt3.research.microsoft.com/ICLRTML2020/</a></p>
    
  <p>The workshop will include contributed papers. Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or a poster presentation (with a lightning talk).</p>
  <p>Submitted papers are expected to introduce novel ideas or results. Submissions should follow the ICLR format and not exceed 4 pages (excluding references, appendices or large figures).</p>
  <p><b>
    Work that has been previously published (including in the ICLR 2020 main conference) will not be accepted at the workshop.
  </b></p>
  <p>We invite submissions on <b>any aspect of machine learning that relates to computer security (and vice versa)</b>. This includes, but is not limited to:</p>
  <ul>
      <li><b>Adversarial Robustness:</b> New approaches that may be risky and are different than the existing literature. Reviewers will pay special attention to the stated threat models and its motivation. Threat models beyond Lp norms are encouraged.  </li>
      <li><b>Real-world attacks:</b> Apply an existing known (academic) threat to a deployed-in-production system to show how it fails.</li>
<li><b>Training time attacks and defenses</b> Develop new approaches that study the threat model where the adversary has access to the training data or algorithm.</li>     
      <li><b>Evaluating privacy of models:</b> Better and broader quantification methods to measure to what extent models trained on sensitive data reveal their training data.</li>
      <li><b>ML algorithms for private learning:</b> new ML models or algorithms that are better suited for privacy-preserving techniques, rather than retroactively adapt existing ML algorithms to be private.</li>
      <li><b>Alternate uses of secure and private learning:</b> Evaluate other benefits of training models to be robust or private. </li>
      <li><b>Unintended consequences of secure or private learning</b> Find unintended consequences of training robust or private models, e.g., on fairness.</li>
      <li><b>Evaluating stealing robustness</b> New methods to quantify the difficulty of stealing trained ML models and develop defenses against it.</li>
      <li><b>Ethical machine learning</b> Definitions and applications of ethics when considering security and privacy aspects in machine learning.</li>
<li><b>Fresh look on incentives in ML:</b> Solutions that consider the model owner as adversarial and attempt to increase privacy, fairness, equality, etc “from the outside” of the model.</li>
<li><b>Foundations for secure or private learning:</b> Introduce proposals for formal foundations of secure or private learning </li>
 <li><b>Position papers:</b> State a new controversial positions or a research agendas that areis under-studied.</li>
      
            
  </ul>
  <p>When relevant, submissions are encouraged to clearly state their <b>threat model</b>, release <b>open-source code</b> and take particular care in conducting <b>ethical research</b>. Reviewing will be performed in a <b>single-blind</b> fashion (reviewers will be anonymous but not authors). Reviewing criteria include (a) <b>relevance</b>, (b) <b>quality</b> of the methodology and experiments, (c) <b>originality</b>.</p>
<!--  <p>Note that submissions on privacy would be best submitted to the <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10934" target="_blank">workshop dedicated to this topic</a>.</p>-->
  <p>This workshop will not have proceedings.</p>
  <p>Contact trustworthyiclr20@googlegroups.com for any questions.</p>




</main>



</body></html>
